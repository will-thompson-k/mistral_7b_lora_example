# mistral_7b_lora_example

A straightforward example illustrating how to fine-tune Mistal 7b with [QLoRA](https://arxiv.org/abs/2305.14314).

Derived from [this blogpost](https://blog.neuralwork.ai/an-llm-fine-tuning-cookbook-with-mistral-7b/).

*Work In Progress*.

Uses huggingface, trl, peft (via huggingface), bitsandbytes and pytorch (obviously).